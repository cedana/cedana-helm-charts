apiVersion: v1
kind: Pod
metadata:
  name: transformers-inference
  namespace: default
  labels:
    app: transformers-inference
    cedana.ai/node-restore: "true"
spec:
  restartPolicy: Never
  runtimeClassName: cedana # required for GPU C/R support
  containers:
  - name: transformers-inference
    image: cedana/cedana-samples:cuda12.4-torch2.5
    command: ["python3", "-u", "/app/gpu_smr/pytorch/llm/transformers_inference.py"]
    resources:
      limits:
        nvidia.com/gpu: 1
