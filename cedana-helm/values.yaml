nameOverride: "cedana"
fullnameOverride: "cedana"

# Install the following CRDs before setting installKueue as true
# kubectl apply --server-side -f https://github.com/kubernetes-sigs/kueue/releases/download/v0.10.1/manifests.yaml
installKueue: false

config:
  authToken: ""
  url: ""
  clusterId: ""
  sqsQueueUrl: ""

  # Specify the address and port to bind the daemon on
  #
  # Address has to be 0.0.0.0 for the service to be externally addressable,
  # if you don't want it to be feel free to update, it ideally won't affect stability
  #
  # Default: 0.0.0.0:8080
  address: 0.0.0.0:8080
  # Specify the protocol to use for daemon service
  # Default: tcp (other protocols might not be supported properly, update with caution)
  # Options:
  # - tcp: TCP socket
  # - unix: Unix Socket
  # - vsock: VSock for VM and hypervisor communication, useful for supporting VM based migrations
  protocol: tcp

  # Specify path to directory for storing checkpoints.
  # - `cedana://<path>` for Cedana-managed global storage (recommended)
  # - `s3://<bucket>/<path>` for your S3 storage
  # - `<path>` for node-local storage
  checkpointDir: "cedana://"
  # Specify the number of parallel streams to use for streaming checkpoint/restore operations.
  # 0 means no streaming. n > 0 means n parallel streams (or number of pipes) to use.
  checkpointStreams: 4
  checkpointCompression: "lz4" # Options: "none", "tar", "lz4", "gzip", "zlib"

  gpuPoolSize: 0 # Number of GPU controllers to keep warm. Improves GPU workload startup/restore time but uses more memory.
  gpuFreezeType: "IPC" # Default freeze type for GPU workloads. Options: "IPC", "NCCL"
  gpuShmSize: "8589934592" # 8 GiB. Enough for most workloads. Reduce if memory constrained or running small workloads only.
  gpuLdLibPath: "/run/nvidia/driver/usr/lib/x86_64-linux-gnu" # Additional LD_LIBRARY_PATH to look for CUDA libraries

  # Specify the plugin versions to use. If pluginsBuilds is "release", then any release version
  # for the plugins can be specified. If pluginsBuilds is "alpha", then specify the branch name.
  pluginsBuilds: "release"
  pluginsNativeVersion: "latest"
  pluginsCriuVersion: "v4.1.1-cedana.00"
  pluginsRuntimeShimVersion: "v0.6.1"
  pluginsGpuVersion: "v0.5.9"
  pluginsStreamerVersion: "v0.0.8"

  profiling: true
  metrics: true
  logLevel: "info"

  awsAccessKeyId: "" # AWS access key ID if using S3 storage (if `s3://<bucket>/<path>` in checkpointDir)
  awsSecretAccessKey: "" # AWS secret access key if using S3 storage
  awsRegion: "" # AWS region if using S3 storage (uses default region if not set)

  # Uncomment, to use custom pre-existing secret
  # preExistingSecret: cedana-secret-user

# Optional configuration to increase /dev/shm size on nodes
# This is useful for workloads that require large shared memory
shmConfig:
  enabled: false # Set to true to enable /dev/shm size increase
  size: "10G" # Size to set for /dev/shm (e.g., "10G", "20G")
  minSize: "10G" # Minimum size to trigger remount (e.g., "10G", "20G")

daemonHelper:
  service:
    annotations: {}
  image:
    repository: cedana/cedana-helper
    tag: v0.9.260
    digest: # ignores tag if set
    imagePullPolicy: IfNotPresent
  updateStrategy:
    maxSurge: 0
    maxUnavailable: 99999
  tolerations: []
  affinity: {}
  nodeSelector: {}

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Automatically mount a ServiceAccount's API credentials?
  automount: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: "cedana-controller-manager"

controllerManager:
  autoscaling:
    enabled: false
    replicaCount: 1
    deploymentRevisionHistoryLimit: 10
  service:
    annotations: {}
    ports:
      - protocol: TCP
        port: 1324
        targetPort: 1324
  manager:
    podAnnotations: {}
    args:
      - --health-probe-bind-address=:8081
      - --metrics-bind-address=127.0.0.1:8080
      - --leader-elect
    containerSecurityContext:
      # controller doesn't require any privileges
      allowPrivilegeEscalation: false
      capabilities:
        drop:
          - ALL
    image:
      repository: cedana/cedana-controller
      tag: v0.5.3
      digest: # ignores tag if set
      imagePullPolicy: IfNotPresent
    resources:
      # empty to ensure minimal resource
      # usage on demo/test deployments
      # uncomment or add custom resource
      # limits:
      #   cpu: 500m
      #   memory: 128Mi
      # requests:
      #   cpu: 10m
      #   memory: 64Mi
  rbac:
    resources:
      # empty to ensure minimal resource
      # usage on demo/test deployments
      # uncomment or add custom resource
      # limits:
      #   cpu: 500m
      #   memory: 128Mi
      # requests:
      #   cpu: 10m
      #   memory: 64Mi
  tolerations:
    # Run the command below to deploy the controller on a dedicated node
    # This will not allow shceduling of new pods on the node running the controller pod
    # kubectl taint node <node-name> dedicated=cedana-manager:NoSchedule
    # - key: "dedicated"
    #   operator: "Equal"
    #   value: "cedana-manager"
    #   effect: "NoSchedule"
  affinity:
    # Allow scheduling of controller pod on the labeled node only
    # kubectl label nodes <node-name> dedicated=cedana-manager
    # nodeAffinity:
    #   requiredDuringSchedulingIgnoredDuringExecution:
    #     nodeSelectorTerms:
    #       - matchExpressions:
    #           - key: "dedicated"
    #             operator: "In"
    #             values:
    #               - "cedana-manager"
  nodeSelector: {}

kubernetesClusterDomain: cluster.local

metricsService:
  ports:
    - name: https
      port: 8443
      protocol: TCP
      targetPort: https
  type: ClusterIP
