nameOverride: cedana
fullnameOverride: cedana

# Install the following CRDs before setting installKueue as true
# kubectl apply --server-side -f https://github.com/kubernetes-sigs/kueue/releases/download/v0.10.1/manifests.yaml
installKueue: false

config:
  authToken:
  url:
  clusterId:
  sqsQueueUrl:

  # Specify the address and port to bind the daemon on
  #
  # Address has to be 0.0.0.0 for the service to be externally addressable,
  # if you don't want it to be feel free to update, it ideally won't affect stability
  #
  # Default: 0.0.0.0:8080
  address: 0.0.0.0:8080
  # Specify the protocol to use for daemon service
  # Default: tcp (other protocols might not be supported properly, update with caution)
  # Options:
  # - tcp: TCP socket
  # - unix: Unix Socket
  # - vsock: VSock for VM and hypervisor communication, useful for supporting VM based migrations
  protocol: tcp

  # Specify path to directory for storing checkpoints.
  # - `cedana://<path>` for Cedana-managed global storage (recommended)
  # - `s3://<bucket>/<path>` for your S3 storage
  # - `<path>` for node-local storage
  checkpointDir: cedana://
  # Specify the number of parallel streams to use for streaming checkpoint/restore operations.
  # 0 means no streaming. n > 0 means n parallel streams (or number of pipes) to use.
  checkpointStreams: 4
  checkpointCompression: lz4 # Options: "none", "tar", "lz4", "gzip", "zlib"
  # Specify whether to perform checkpoint compression/upload async. Recommended if anticipating large checkpoints,
  # which could take too long to stream directly to a bucket. Note that restore will only be possible after upload is complete,
  # so there might be a delay before a workload can be restored.
  checkpointAsync: false

  gpuPoolSize: 0 # Number of GPU controllers to keep warm. Improves GPU workload startup/restore time but uses more memory.
  gpuShmSize: 8589934592 # 8 GiB. Enough for most workloads. Reduce if memory constrained or running small workloads only.
  gpuLdLibPath: /run/nvidia/driver/usr/lib/x86_64-linux-gnu # Additional LD_LIBRARY_PATH to look for CUDA libraries

  # Specify the plugin versions to use. If pluginsBuilds is "release", then any release version
  # for the plugins can be specified. If pluginsBuilds is "alpha", then specify the branch name.
  pluginsBuilds: alpha
  pluginsNativeVersion: latest
  pluginsCriuVersion: 7/merge
  pluginsContainerdRuntimeVersion: v0.7.1
  pluginsGpuVersion: v0.7.0
  pluginsStreamerVersion: v0.0.8

  profiling: true
  metrics: true
  logLevel: info

  awsAccessKeyId: # AWS access key ID if using S3 storage (if `s3://<bucket>/<path>` in checkpointDir)
  awsSecretAccessKey: # AWS secret access key if using S3 storage
  awsRegion: # AWS region if using S3 storage (uses default region if not set)
  awsEndpoint: # AWS endpoint if using S3-compatible storage

  containerdAddress: /run/containerd/containerd.sock # Path to containerd socket

  # Uncomment, to use custom pre-existing secret
  # preExistingSecret: cedana-secret-user

# Optional configuration to increase /dev/shm size on nodes
# This is useful for workloads that require large shared memory
shmConfig:
  enabled: false # Set to true to enable /dev/shm size increase
  size: 10G # Size to set for /dev/shm (e.g., "10G", "20G")
  minSize: 10G # Minimum size to trigger remount (e.g., "10G", "20G")

daemonHelper:
  service:
    annotations: {}
  image:
    repository: cedana/cedana-helper
    tag: v0.9.279
    digest: # ignores tag if set
    imagePullPolicy: IfNotPresent
  updateStrategy:
    maxSurge: 0
    maxUnavailable: 99999
  tolerations: []
  affinity: {}
  nodeSelector: {}

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Automatically mount a ServiceAccount's API credentials?
  automount: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: cedana-controller-manager

controllerManager:
  autoscaling:
    enabled: false
    replicaCount: 1
    deploymentRevisionHistoryLimit: 10
  service:
    annotations: {}
    ports:
      - protocol: TCP
        port: 1324
        targetPort: 1324
  manager:
    podAnnotations: {}
    args:
      - --health-probe-bind-address=:8081
      - --metrics-bind-address=127.0.0.1:8080
      - --leader-elect
    containerSecurityContext:
      # controller doesn't require any privileges
      allowPrivilegeEscalation: false
      capabilities:
        drop:
          - ALL
    image:
      repository: cedana/cedana-controller
      tag: v0.6.0
      digest: # ignores tag if set
      imagePullPolicy: IfNotPresent
    resources:
      # empty to ensure minimal resource
      # usage on demo/test deployments
      # uncomment or add custom resource
      # limits:
      #   cpu: 500m
      #   memory: 128Mi
      # requests:
      #   cpu: 10m
      #   memory: 64Mi
  rbac:
    resources:
      # empty to ensure minimal resource
      # usage on demo/test deployments
      # uncomment or add custom resource
      # limits:
      #   cpu: 500m
      #   memory: 128Mi
      # requests:
      #   cpu: 10m
      #   memory: 64Mi
  tolerations:
    # Run the command below to deploy the controller on a dedicated node
    # This will not allow shceduling of new pods on the node running the controller pod
    # kubectl taint node <node-name> dedicated=cedana-manager:NoSchedule
    # - key: "dedicated"
    #   operator: "Equal"
    #   value: "cedana-manager"
    #   effect: "NoSchedule"
  affinity:
    # Allow scheduling of controller pod on the labeled node only
    # kubectl label nodes <node-name> dedicated=cedana-manager
    # nodeAffinity:
    #   requiredDuringSchedulingIgnoredDuringExecution:
    #     nodeSelectorTerms:
    #       - matchExpressions:
    #           - key: "dedicated"
    #             operator: "In"
    #             values:
    #               - "cedana-manager"
  nodeSelector: {}

kubernetesClusterDomain: cluster.local

metricsService:
  ports:
    - name: https
      port: 8443
      protocol: TCP
      targetPort: https
  type: ClusterIP
